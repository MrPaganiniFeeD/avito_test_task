{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] ds1: copied=1863, missing_label=0\n",
      "[INFO] ds2: no val/images — пропускаю val\n",
      "[DONE] ds2: copied=859, missing_label=0\n",
      "[DONE] ds3: copied=180, missing_label=0\n",
      "[DONE] ds4: copied=2094, missing_label=0\n",
      "[DONE] ds5: copied=894, missing_label=0\n",
      "[DONE] ds6: copied=1035, missing_label=0\n",
      "[DONE] ds7: copied=600, missing_label=0\n",
      "[INFO] ds8: no val/images — пропускаю val\n",
      "[DONE] ds8: copied=2079, missing_label=0\n",
      "[INFO] ds9: no val/images — пропускаю val\n",
      "[DONE] ds9: copied=2025, missing_label=0\n",
      "=== SUMMARY ===\n",
      "ds1 {'copied': 1863, 'missing_label': 0, 'no_split': 0}\n",
      "ds2 {'copied': 859, 'missing_label': 0, 'no_split': 1}\n",
      "ds3 {'copied': 180, 'missing_label': 0, 'no_split': 0}\n",
      "ds4 {'copied': 2094, 'missing_label': 0, 'no_split': 0}\n",
      "ds5 {'copied': 894, 'missing_label': 0, 'no_split': 0}\n",
      "ds6 {'copied': 1035, 'missing_label': 0, 'no_split': 0}\n",
      "ds7 {'copied': 600, 'missing_label': 0, 'no_split': 0}\n",
      "ds8 {'copied': 2079, 'missing_label': 0, 'no_split': 1}\n",
      "ds9 {'copied': 2025, 'missing_label': 0, 'no_split': 1}\n",
      "Combined folder ready at: dataset\\combined\n"
     ]
    }
   ],
   "source": [
    "# merge_datasets_specific.py\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"data\")\n",
    "datasetPath = Path(\"dataset\")\n",
    "datasets = [\n",
    "    root / \"all-books\",\n",
    "    root / \"book\",\n",
    "    root / \"Book_200img\",\n",
    "    root / \"book_detection2\",\n",
    "    root / \"book_1\",\n",
    "    root / \"book_2\",\n",
    "    root / \"book_3\",\n",
    "    root / \"book_4\",\n",
    "    root / \"book_5\"\n",
    "]\n",
    "\n",
    "out = datasetPath / \"combined\"\n",
    "imgs_out = out / \"images\"\n",
    "lbls_out = out / \"labels\"\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    (imgs_out / split).mkdir(parents=True, exist_ok=True)\n",
    "    (lbls_out / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "\n",
    "def copy_pair(src_img, src_lbl, dest_img, dest_lbl):\n",
    "    dest_img.parent.mkdir(parents=True, exist_ok=True)\n",
    "    dest_lbl.parent.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(src_img, dest_img)\n",
    "    shutil.copy2(src_lbl, dest_lbl)\n",
    "\n",
    "summary = {}\n",
    "for i, ds in enumerate(datasets, start=1):\n",
    "    prefix = f\"ds{i}\"\n",
    "    summary[prefix] = {\"copied\":0, \"missing_label\":0, \"no_split\":0}\n",
    "    if not ds.exists():\n",
    "        print(f\"[WARN] dataset not found: {ds} — skipping\")\n",
    "        continue\n",
    "\n",
    "    for split in [\"train\",\"val\"]:\n",
    "        imgs_dir = ds / split / \"images\"\n",
    "        lbls_dir = ds / split / \"labels\"\n",
    "        if not imgs_dir.exists():\n",
    "            print(f\"[INFO] {prefix}: no {split}/images — пропуск {split}\")\n",
    "            summary[prefix][\"no_split\"] += 1\n",
    "            continue\n",
    "        img_files = [p for p in imgs_dir.iterdir() if p.suffix.lower() in IMG_EXTS]\n",
    "        for img in img_files:\n",
    "            lbl = lbls_dir / f\"{img.stem}.txt\"\n",
    "            if not lbl.exists():\n",
    "                summary[prefix][\"missing_label\"] += 1\n",
    "                continue\n",
    "            new_img = imgs_out / split / f\"{prefix}_{img.name}\"\n",
    "            new_lbl = lbls_out / split / f\"{prefix}_{lbl.name}\"\n",
    "            copy_pair(img, lbl, new_img, new_lbl)\n",
    "            summary[prefix][\"copied\"] += 1\n",
    "\n",
    "    print(f\"[DONE] {prefix}: copied={summary[prefix]['copied']}, missing_label={summary[prefix]['missing_label']}\")\n",
    "\n",
    "print(\"=== SUMMARY ===\")\n",
    "for k,v in summary.items():\n",
    "    print(k, v)\n",
    "print(\"Combined folder ready at:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: добавлено 5332 изображений с книгами\n",
      "val: добавлено 230 изображений с книгами\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "coco_path = \"data/coco2017\"\n",
    "combined_path = \"dataset/combined\"\n",
    "\n",
    "splits = {\"train2017\": \"train\", \"val2017\": \"val\"}\n",
    "\n",
    "for coco_split, yolo_split in splits.items():\n",
    "    ann_file = os.path.join(coco_path, \"annotations\", f\"instances_{coco_split}.json\")\n",
    "    with open(ann_file, \"r\") as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "    book_id = 84  # id \"book\" в COCO\n",
    "\n",
    "    anns_book = [ann for ann in coco[\"annotations\"] if ann[\"category_id\"] == book_id]\n",
    "\n",
    "    img_ids = set([a[\"image_id\"] for a in anns_book])\n",
    "    imgs_book = [img for img in coco[\"images\"] if img[\"id\"] in img_ids]\n",
    "\n",
    "    img_save_dir = os.path.join(combined_path, \"images\", yolo_split)\n",
    "    lbl_save_dir = os.path.join(combined_path, \"labels\", yolo_split)\n",
    "    os.makedirs(img_save_dir, exist_ok=True)\n",
    "    os.makedirs(lbl_save_dir, exist_ok=True)\n",
    "\n",
    "    src_img_dir = os.path.join(coco_path, coco_split)\n",
    "\n",
    "    for img in imgs_book:\n",
    "        img_id = img[\"id\"]\n",
    "        w, h = img[\"width\"], img[\"height\"]\n",
    "\n",
    "        anns = [a for a in anns_book if a[\"image_id\"] == img_id]\n",
    "\n",
    "        base_name = f\"coco_{img['file_name']}\"\n",
    "        dst_img = os.path.join(img_save_dir, base_name)\n",
    "        src_img = os.path.join(src_img_dir, img[\"file_name\"])\n",
    "        shutil.copyfile(src_img, dst_img)\n",
    "\n",
    "        lines = []\n",
    "        for a in anns:\n",
    "            x, y, bw, bh = a[\"bbox\"]\n",
    "            xc = (x + bw / 2) / w\n",
    "            yc = (y + bh / 2) / h\n",
    "            bw /= w\n",
    "            bh /= h\n",
    "            lines.append(f\"0 {xc:.6f} {yc:.6f} {bw:.6f} {bh:.6f}\")\n",
    "\n",
    "        label_file = os.path.join(lbl_save_dir, base_name.replace(\".jpg\", \".txt\"))\n",
    "        with open(label_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(lines))\n",
    "\n",
    "    print(f\"{yolo_split}: добавлено {len(imgs_book)} изображений с книгами\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote dataset\\combined\\data.yaml\n"
     ]
    }
   ],
   "source": [
    "# make_combined_data_yaml.py\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"dataset\")\n",
    "combined = root / \"combined\"\n",
    "data_yaml = combined / \"data.yaml\"\n",
    "\n",
    "candidates = list(root.rglob(\"data.yaml\"))\n",
    "names = [\"book\"]\n",
    "for p in candidates:\n",
    "    try:\n",
    "        d = yaml.safe_load(p.read_text())\n",
    "        if isinstance(d, dict) and 'names' in d:\n",
    "            names = d['names']\n",
    "            print(f\"[INFO] Found names in {p}\")\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if isinstance(names, dict):\n",
    "    items = sorted(names.items(), key=lambda x: int(x[0]))\n",
    "    names = [v for k,v in items]\n",
    "\n",
    "if names is None:\n",
    "    names = [\"book\"]  \n",
    "\n",
    "data = {\n",
    "    'train': \"../images/train\",\n",
    "    'val': \"../images/val\",\n",
    "    'names': names\n",
    "}\n",
    "\n",
    "data_yaml.parent.mkdir(parents=True, exist_ok=True)\n",
    "data_yaml.write_text(yaml.safe_dump(data, sort_keys=False, allow_unicode=True))\n",
    "print(\"Wrote\", data_yaml)\n",
    "if not names:\n",
    "    print(\"WARNING: 'names' is empty\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
